{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCHP4lw5cpFt"
      },
      "source": [
        "# **Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZOQNQsLcwBd"
      },
      "outputs": [],
      "source": [
        "!pip3 install pickle5\n",
        "import pickle5 as pc\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "%matplotlib inline\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "# set random seed for random weights\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)\n",
        "data_train = pd.read_pickle('data_train') #full set\n",
        "data_train = data_train[data_train.ID <= 100] # Change this for 100, 500 or 1000 training structures\n",
        "data_val = pd.read_pickle('data_val')\n",
        "data_test = pd.read_pickle('data_test')",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "\n",
        "ax.plot(data_train[data_train.ID == 1]['cycle'],data_train[data_train.ID == 1]['gauge1'],label= 'gauge1')\n",
        "ax.plot(data_train[data_train.ID == 1]['cycle'],data_train[data_train.ID == 1][ 'gauge2'],label= 'gauge2')\n",
        "ax.plot(data_train[data_train.ID == 1]['cycle'],data_train[data_train.ID == 1]['gauge3'], label= 'gauge3')\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "\n",
        "ax.set(xlabel='Cycle', ylabel='Strain')\n",
        "\n",
        "plt.savefig('training_data_ID_1.pdf', bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.plot(data_test[data_test.ID == 1]['gauge1'],label= 'gauge1')\n",
        "plt.plot(data_test[data_test.ID == 1][ 'gauge2'],label= 'gauge2')\n",
        "plt.plot(data_test[data_test.ID == 1]['gauge3'], label= 'gauge3')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CsVUHwSdS4O"
      },
      "source": [
        "# **Categorize Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x14NTX7ydV5q"
      },
      "outputs": [],
      "source": [
        "print(data_train.RUL.describe())\n",
        "print(data_test.RUL.describe())\n",
        "\n",
        "l=0.5\n",
        "nb_bins = 20\n",
        "lower_bin_bound = 0\n",
        "upper_bin_bound = 80000\n",
        "\n",
        "bins = np.linspace(lower_bin_bound, upper_bin_bound**(1-l), nb_bins)**(1/l)\n",
        "labels=[i for i in range(bins.shape[0]-1)]\n",
        "plt.plot(bins,np.full(bins.shape[0],0),'o')\n",
        "print(bins)\n",
        "print(labels)\n",
        "\n",
        "data_train['RUL_bins'] = pd.cut(data_train['RUL'], bins=bins, labels=labels)\n",
        "data_test['RUL_bins'] = pd.cut(data_test['RUL'], bins=bins, labels=labels)\n",
        "\n",
        "display(data_train)\n",
        "display(data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCmGsaNLdewa"
      },
      "source": [
        "# **Build Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdNKCIFXdhYR"
      },
      "outputs": [],
      "source": [
        "#utils \n",
        "nb_gauges = 3\n",
        
        "\n",
        "#prepare forecasting data\n",
        "def gen_X_sequence(id_df, seq_length, seq_cols,timesteps_pred,  type_data = None):\n",
        
        "    ind_start = 0\n",
        "    \n",
        "    data_array = id_df[seq_cols].values\n",
        "    num_elements = data_array.shape[0]\n",
        "    for start, stop in zip(range(0+ind_start, num_elements-seq_length+1-timesteps_pred), range(seq_length+ind_start, num_elements+1-timesteps_pred)):\n",
        "        yield data_array[start:stop, :]\n",
        " \n",
        "\n",
        "def gen_Y_sequence(id_df, seq_length, seq_cols,timesteps_pred, type_data = None):\n",
        
        "    ind_start = 0\n",
        "    \n",
        "    data_array = id_df[seq_cols].values\n",
        "    num_elements = data_array.shape[0]\n",
        "    for start, stop in zip(range(0+ind_start, num_elements-seq_length+1-timesteps_pred), range(seq_length+ind_start, num_elements+1-timesteps_pred)):\n",
        "        yield data_array[stop-1, :]#data_array[start+1:stop+1, :]\n",
        "   \n",
        "#prepare data\n",
        "seq_cols =  ['gauge'+str(i) for i in range(1,4)]#['label'+str(i) for i in range(1,4)]\n",
        "seq_cols1 =  ['RUL_bins']\n",
        "sequence_length = 30\n",
        "timesteps_pred = 1\n",
        "\n",
        "\n",
        "\n",
        "#training set\n",
        "seq_gen = (list(gen_X_sequence(data_tr[data_tr['ID']==id], sequence_length, seq_cols, timesteps_pred, type_data= 'train')) \n",
        "                   for id in data_tr['ID'].unique())\n",
        "# generate sequences and convert to numpy array\n",
        "dbX = np.concatenate(list(seq_gen))\n",
        "\n",
        "\n",
        "\n",
        "seq_gen = (list(gen_Y_sequence(data_tr[data_tr['ID']==id], sequence_length, seq_cols1, timesteps_pred, type_data= 'train')) \n",
        "                   for id in data_tr['ID'].unique())\n",
        "# generate sequences and convert to numpy array\n",
        "dbY = np.concatenate(list(seq_gen))#.astype(np.long).reshape(-1,)\n",
        "\n",
        "\n",
        "print(dbX.shape)\n",
        "print(dbY.shape)\n",
        "\n",
        "#validation set\n",
        "seq_gen = (list(gen_X_sequence(data_val[data_val['ID']==id], sequence_length, seq_cols, timesteps_pred, type_data= 'train')) \n",
        "                   for id in data_val['ID'].unique())\n",
        "\n",
        "# generate sequences and convert to numpy array\n",
        "dbX_val = np.concatenate(list(seq_gen))#.astype(np.long)\n",
        "\n",
        "seq_gen = (list(gen_Y_sequence(data_val[data_val['ID']==id], sequence_length, seq_cols1, timesteps_pred, type_data= 'train')) \n",
        "                   for id in data_val['ID'].unique())\n",
        "# generate sequences and convert to numpy array\n",
        "dbY_val = np.concatenate(list(seq_gen))#.astype(np.long).reshape(-1,)\n",
        "\n",
        "\n",
        "#test set\n",
        "dbX_test = [data_test[data_test['ID']==id][seq_cols].values[-sequence_length:] for id in data_test['ID'].unique()]\n",
        "dbX_test = np.asarray(dbX_test)\n",
        "\n",
        "dbY_test = [data_test[data_test['ID']==id][seq_cols1].values[-1] for id in data_test['ID'].unique()]\n",
        "dbY_test = np.asarray(dbY_test)\n",
        "\n",
        "\n",
        "\n",
        "print(dbX_val.shape)\n",
        "print(dbY_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzt2sHlldzTC"
      },
      "source": [
        "# **Normalize Data and One Hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZP78lQ2d3vC"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "dbY = to_categorical(dbY,nb_bins)\n",
        "dbY_val = to_categorical(dbY_val,nb_bins)\n",
        "dbY_test = to_categorical(dbY_test,nb_bins)\n",
        "\n",
        "print(dbX.shape)\n",
        "dbX[:,:,0].max()\n",
        "for k in range(3) :\n",
        "  a = dbX[:,:,k].max()\n",
        "  b = dbX[:,:,k].min()\n",
        "  dbX[:,:,k] = (dbX[:,:,k] - b)/(a-b)\n",
        "  dbX_val[:,:,k] = (dbX_val[:,:,k] - b)/(a-b)\n",
        "  dbX_test[:,:,k] = (dbX_test[:,:,k] - b)/(a-b)\n",
        "\n",
        "\n",
        "dbX\n",
        "dbX_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHIqUWkLrKb_"
      },
      "source": [
        "# **Build the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfJzvnQFEG_q"
      },
      "outputs": [],
      "source": [
        "dbY.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFS2SpWuqFaS"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import *\n",
        "\n",
        "\n",
        "#model_path = ''\n",
        "import json\n",
        "json_file = open(model_path + '/GRU_100_tuned.json')\n",
        "loaded_model = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model)\n",
        "model.load_weights(model_path + '/GRU_100_tuned.h5')\n",
        "model.get_config()\n",
        "model.summary()\n",
        "model.get_layer(index=3).recurrent_dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu0oxHF6C8KB"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers.recurrent import LSTM, GRU, RNN\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "\n",
        "\n",
        "history_seq = Input(shape = (dbX.shape[1],dbX.shape[2]))\n",
        "x = history_seq\n",
        "x = GRU(32, return_sequences=True, dropout=0, recurrent_dropout=0.1)(x)\n",
        "x = GRU(32, return_sequences=True, dropout=0, recurrent_dropout=0.1)(x)\n",
        "x = GRU(32, return_sequences=False, dropout=0, recurrent_dropout=0.1)(x)\n",
        "x = Activation('relu')(x)\n",
        "x = Dense(dbY.shape[1], activation = 'softmax')(x)\n",
        "model = Model(history_seq, x)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH88Cuy_FO2_"
      },
      "outputs": [],
      "source": [
        "dbX.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwiBgfvzru7p"
      },
      "source": [
        "# **Compile and train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2naH2sJtDGAN"
      },
      "outputs": [],
      "source": [
        "dbY.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALeeOCCNFMn5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivER7BdEryxq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "List_lr=[0.01, 0.001, 0.0001, 0.00001]\n",
        "learning_rate = List_lr[0]\n",
        "lr_step = '0'\n",
        "i=0\n",
        "opt = Adam(learning_rate=learning_rate)\n",
        "\n",
        "List_hist=[]\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "temp_model = model\n",
        "\n",
        "for i in range (0,len(List_lr)):\n",
        "\n",
        "\n",
        "  ES = keras.callbacks.EarlyStopping(monitor='accuracy', min_delta=0, patience=20, verbose=2, mode='max')\n",
        "  mc = keras.callbacks.ModelCheckpoint('best_model_gru_prob.h5', monitor='accuracy', mode='max', save_best_only=True)\n",
        "\n",
        "  lr_step = str(i)\n",
        "  batch_size = 4096\n",
        "  epochs = 500\n",
        "  K.set_value(temp_model.optimizer.learning_rate, learning_rate)\n",
        "  history = temp_model.fit(dbX, dbY, batch_size=batch_size, epochs=epochs, validation_data=(dbX_val, dbY_val), callbacks=[mc])\n",
        "  List_hist.append(history.history['accuracy'])\n",
        "  List_hist.append(history.history['val_accuracy'])\n",
        "  List_hist.append(history.history['loss'])\n",
        "  List_hist.append(history.history['val_loss'])\n",
        "  lr_step = str(i)\n",
        "  learning_rate = List_lr[i]\n",
        "  temp_model=tf.keras.models.load_model('best_model_gru_prob.h5')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmuf8hafrhmF"
      },
      "source": [
        "# **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBS4HKZEYfF0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import keras.backend as K\n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "model.evaluate(dbX_test, dbY_test, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L90RGi-7rno9"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/PIR_perso/models_GRU/100_structures'\n",
        "# serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(model_path + \"/GRU_100_tuned.json\", \"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(model_path + \"/GRU_100_tuned.h5\")\n",
        "\n",
        "\n",
        "\n",
        "model.evaluate(dbX_test, dbY_test, batch_size=32)\n",
        "# summarize history for accuracy\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "acc = List_hist[0] + List_hist[4] + List_hist[8] + List_hist[12]        # concatenate history for the 3 learning rate to make it a single learning\n",
        "val_acc = List_hist[1] + List_hist[5] + List_hist[9] + List_hist[13]\n",
        "loss = List_hist[2] + List_hist[6] + List_hist[10] + List_hist[14]\n",
        "val_loss = List_hist[3] + List_hist[7] + List_hist[11] + List_hist[15]\n",
        "plt.plot(acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "fig.savefig(model_path + '/GRU_100_tuned_acc.png')\n",
        "# summarize history for loss\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "plt.plot(loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "fig.savefig(model_path + '/GRU_100_tuned_loss.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GRU_100_Fine.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
