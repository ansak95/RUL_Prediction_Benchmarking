{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCHP4lw5cpFt"
      },
      "source": [
        "# **Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZOQNQsLcwBd"
      },
      "outputs": [],
      "source": [
        "!pip3 install pickle5\n",
        "import pickle5 as pc\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pandas.io.json import json_normalize\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "%matplotlib inline\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "\n",
        "# set random seed for random weights\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)\n",
        "fd=''\n",
        "fk='../../data'\n",
        "with open(fd + fk + '/data_train_v1', \"rb\") as fh:\n",
        "  data_train = pc.load(fh).reset_index().iloc[:,1:]\n",
        "with open(fd + fk +'/data_test_v1', \"rb\") as fh:\n",
        "  data_test = pc.load(fh).reset_index().iloc[:,1:]\n",
        "\n",
        "#data_train[data_train.ID == 1]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(7,5))\n",
        "\n",
        "ax.plot(data_train[data_train.ID == 1]['cycle'],data_train[data_train.ID == 1]['gauge1'],label= 'gauge1')\n",
        "ax.plot(data_train[data_train.ID == 1]['cycle'],data_train[data_train.ID == 1][ 'gauge2'],label= 'gauge2')\n",
        "ax.plot(data_train[data_train.ID == 1]['cycle'],data_train[data_train.ID == 1]['gauge3'], label= 'gauge3')\n",
        "ax.grid()\n",
        "ax.legend()\n",
        "\n",
        "ax.set(xlabel='Cycle', ylabel='Strain')\n",
        "\n",
        "plt.savefig('training_data_ID_1.pdf', bbox_inches='tight')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.plot(data_test[data_test.ID == 1]['gauge1'],label= 'gauge1')\n",
        "plt.plot(data_test[data_test.ID == 1][ 'gauge2'],label= 'gauge2')\n",
        "plt.plot(data_test[data_test.ID == 1]['gauge3'], label= 'gauge3')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CsVUHwSdS4O"
      },
      "source": [
        "# **Categorize Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x14NTX7ydV5q"
      },
      "outputs": [],
      "source": [
        "print(data_train.RUL.describe())\n",
        "print(data_test.RUL.describe())\n",
        "\n",
        "l=0.5\n",
        "nb_bins = 20\n",
        "lower_bin_bound = 0\n",
        "upper_bin_bound = 80000\n",
        "\n",
        "bins = np.linspace(lower_bin_bound, upper_bin_bound**(1-l), nb_bins)**(1/l)\n",
        "labels=[i for i in range(bins.shape[0]-1)]\n",
        "plt.plot(bins,np.full(bins.shape[0],0),'o')\n",
        "print(bins)\n",
        "print(labels)\n",
        "\n",
        "data_train['RUL_bins'] = pd.cut(data_train['RUL'], bins=bins, labels=labels)\n",
        "data_test['RUL_bins'] = pd.cut(data_test['RUL'], bins=bins, labels=labels)\n",
        "\n",
        "display(data_train)\n",
        "display(data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCmGsaNLdewa"
      },
      "source": [
        "# **Build Sequences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdNKCIFXdhYR"
      },
      "outputs": [],
      "source": [
        "#utils \n",
        "nb_gauges = 3\n",
        "data_tr = data_train[data_train.ID <= 100]   #500, 1000\n",
        "data_val = data_test[data_test.ID <= 100]\n",
        "\n",
        "#prepare forecasting data\n",
        "def gen_X_sequence(id_df, seq_length, seq_cols,timesteps_pred,  type_data = None):\n",
        "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
        "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
        "    we can use shorter ones \"\"\"\n",
        "    \n",
        "    \n",
        "    ind_start = 0\n",
        "    \n",
        "    data_array = id_df[seq_cols].values\n",
        "    num_elements = data_array.shape[0]\n",
        "    for start, stop in zip(range(0+ind_start, num_elements-seq_length+1-timesteps_pred), range(seq_length+ind_start, num_elements+1-timesteps_pred)):\n",
        "        yield data_array[start:stop, :]\n",
        " \n",
        "\n",
        "def gen_Y_sequence(id_df, seq_length, seq_cols,timesteps_pred, type_data = None):\n",
        "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
        "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
        "    we can use shorter ones \"\"\"\n",
        "    \n",
        "    \n",
        "    ind_start = 0\n",
        "    \n",
        "    data_array = id_df[seq_cols].values\n",
        "    num_elements = data_array.shape[0]\n",
        "    for start, stop in zip(range(0+ind_start, num_elements-seq_length+1-timesteps_pred), range(seq_length+ind_start, num_elements+1-timesteps_pred)):\n",
        "        yield data_array[stop-1, :]#data_array[start+1:stop+1, :]\n",
        "   \n",
        "#prepare data\n",
        "seq_cols =  ['gauge'+str(i) for i in range(1,4)]#['label'+str(i) for i in range(1,4)]\n",
        "seq_cols1 =  ['RUL_bins']\n",
        "sequence_length = 30\n",
        "timesteps_pred = 1\n",
        "\n",
        "\n",
        "\n",
        "#training set\n",
        "seq_gen = (list(gen_X_sequence(data_tr[data_tr['ID']==id], sequence_length, seq_cols, timesteps_pred, type_data= 'train')) \n",
        "                   for id in data_tr['ID'].unique())\n",
        "# generate sequences and convert to numpy array\n",
        "dbX = np.concatenate(list(seq_gen))\n",
        "\n",
        "\n",
        "\n",
        "seq_gen = (list(gen_Y_sequence(data_tr[data_tr['ID']==id], sequence_length, seq_cols1, timesteps_pred, type_data= 'train')) \n",
        "                   for id in data_tr['ID'].unique())\n",
        "# generate sequences and convert to numpy array\n",
        "dbY = np.concatenate(list(seq_gen))#.astype(np.long).reshape(-1,)\n",
        "\n",
        "\n",
        "print(dbX.shape)\n",
        "print(dbY.shape)\n",
        "\n",
        "# validation set\n",
        "seq_gen = (list(gen_X_sequence(data_val[data_val['ID']==id], sequence_length, seq_cols, timesteps_pred, type_data= 'train')) \n",
        "                   for id in data_val['ID'].unique())\n",
        "\n",
        "# generate sequences and convert to numpy array\n",
        "dbX_val = np.concatenate(list(seq_gen))#.astype(np.long)\n",
        "\n",
        "seq_gen = (list(gen_Y_sequence(data_val[data_val['ID']==id], sequence_length, seq_cols1, timesteps_pred, type_data= 'train')) \n",
        "                   for id in data_val['ID'].unique())\n",
        "# generate sequences and convert to numpy array\n",
        "dbY_val = np.concatenate(list(seq_gen))#.astype(np.long).reshape(-1,)\n",
        "\n",
        "#test set\n",
        "seq_gen = (list(gen_X_sequence(data_test[data_test['ID']==id], sequence_length, seq_cols, timesteps_pred, type_data= 'train')) \n",
        "                    for id in data_test['ID'].unique())\n",
        "# generate sequences and convert to numpy array\n",
        "dbX_test = np.concatenate(list(seq_gen))\n",
        "\n",
        "seq_gen = (list(gen_Y_sequence(data_test[data_test['ID']==id], sequence_length, seq_cols1, timesteps_pred, type_data= 'train')) \n",
        "                    for id in data_test['ID'].unique())\n",
        "# generate sequences and convert to numpy array\n",
        "dbY_test = np.concatenate(list(seq_gen))\n",
        "\n",
        "\n",
        "\n",
        "print(dbX_val.shape)\n",
        "print(dbY_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzt2sHlldzTC"
      },
      "source": [
        "# **Normalize Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZP78lQ2d3vC"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(dbX.shape)\n",
        "dbX[:,:,0].max()\n",
        "for k in range(3) :\n",
        "  a = dbX[:,:,k].max()\n",
        "  b = dbX[:,:,k].min()\n",
        "  dbX[:,:,k] = (dbX[:,:,k] - b)/(a-b)\n",
        "  dbX_val[:,:,k] = (dbX_val[:,:,k] - b)/(a-b)\n",
        "  dbX_test[:,:,k] = (dbX_val[:,:,k] - b)/(a-b)\n",
        "\n",
        "\n",
        "dbX\n",
        "dbX_val\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((dbX, dbY)).batch(batch_size)\n",
        "# val_dataset = tf.data.Dataset.from_tensor_slices((dbX_val, dbY_val)).batch(batch_size)\n",
        "# test_dataset = tf.data.Dataset.from_tensor_slices((dbX_test, dbY_test)).batch(batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHIqUWkLrKb_"
      },
      "source": [
        "# **Build the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfJzvnQFEG_q"
      },
      "outputs": [],
      "source": [
        "dbY.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFS2SpWuqFaS"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "import kerastuner\n",
        "from kerastuner.tuners import *\n",
        "from kerastuner import HyperModel\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "\n",
        "\n",
        "class MyHyperModel(HyperModel):\n",
        "\n",
        "    def __init__(self, input_shape, output_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "\n",
        "    def build(self, hp):\n",
        "        # build model\n",
        "        input_layer = Input(shape=(dbX.shape[1],dbX.shape[2]))\n",
        "        x = LayerNormalization(axis=1)(input_layer)\n",
        "\n",
        "        for i in range(hp.Int('num_layers', min_value=0, max_value=2, step=1)):\n",
        "            x = GRU(units=hp.Choice('units', values=[32, 64, 128, 256]), \n",
        "                     dropout=hp.Choice('Dropout',values=[0.0, 0.1]),\n",
        "                     recurrent_dropout=hp.Choice('Recurrent_Dropout',values=[0.0, 0.1]), return_sequences=True)(x)\n",
        "        x = GRU(units=hp.Choice('units', values=[32, 64, 128, 256]), \n",
        "                 dropout=hp.Choice('Dropout',values=[0.0, 0.1]), \n",
        "                 recurrent_dropout=hp.Choice('Recurrent_Dropout',values=[0.0, 0.1]))(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        x = Dense(20, activation='softmax')(x)\n",
        "        output_layer = x\n",
        "\n",
        "        model = Model(input_layer, output_layer)\n",
        "\n",
        "        # compile model\n",
        "        model.compile(\n",
        "            optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['SparseCategoricalAccuracy'])\n",
        "\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwiBgfvzru7p"
      },
      "source": [
        "# **Compile and train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2naH2sJtDGAN"
      },
      "outputs": [],
      "source": [
        "dbY.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALeeOCCNFMn5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivER7BdEryxq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.initializers import *\n",
        "from kerastuner.tuners import *\n",
        "from kerastuner import HyperModel\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 4096\n",
        "sequence_length = 15\n",
        "\n",
        "\n",
        "# # Create a MirroredStrategy.\n",
        "# strategy = tf.distribute.MirroredStrategy()\n",
        "# print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n",
        "\n",
        "# # Open a strategy scope.\n",
        "# with strategy.scope():\n",
        "#     # Everything that creates variables should be under the strategy scope.\n",
        "#     # In general this is only model construction & `compile()`.\n",
        "hypermodel = MyHyperModel(input_shape = (dbX.shape[0], dbX.shape[1]), output_shape = dbY.shape[1])\n",
        "tuner = RandomSearch(\n",
        "    hypermodel,\n",
        "    overwrite=True,\n",
        "    objective='val_sparse_categorical_accuracy',\n",
        "    max_trials=20)\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "\n",
        "val_dataset=tf.data.Dataset.from_tensor_slices((dbX_val, dbY_val)).batch(batch_size)\n",
        "train_dataset=tf.data.Dataset.from_tensor_slices((dbX, dbY)).batch(batch_size)\n",
        "\n",
        "tuner.search(train_dataset,\n",
        "             epochs=200,\n",
        "             verbose=2,\n",
        "             validation_data=val_dataset,\n",
        "             #callbacks=[tf.keras.callbacks.TensorBoard(tuner.directory + '/' + tuner.project_name)],\n",
        "             )\n",
        "\n",
        "models = tuner.get_best_models(num_models=1)\n",
        "\n",
        "tuner.results_summary()\n",
        "\n",
        "best_model = models[0]\n",
        "\n",
        "model_path = tuner.directory + '/' + tuner.project_name + '/' + 'best_model'\n",
        "\n",
        "# get model as json string and save to file\n",
        "model_as_json = best_model.to_json()\n",
        "with open(model_path + '.json', \"w\") as json_file:\n",
        "    json_file.write(model_as_json)\n",
        "# save model weights\n",
        "best_model.save_weights(model_path + '_weights.h5')\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmuf8hafrhmF"
      },
      "source": [
        "# **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L90RGi-7rno9"
      },
      "outputs": [],
      "source": [
        "model.evaluate(dbX_val, dbY_val, batch_size=32)\n",
        "# summarize history for accuracy\n",
        "acc = List_hist[0] + List_hist[4] + List_hist[8]        # concatenate history for the 3 learning rate to make it a single learning\n",
        "val_acc = List_hist[1] + List_hist[5] + List_hist[9]\n",
        "loss = List_hist[2] + List_hist[6] + List_hist[10]\n",
        "val_loss = List_hist[3] + List_hist[7] + List_hist[11]\n",
        "plt.plot(acc)\n",
        "plt.plot(val_acc)\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(loss)\n",
        "plt.plot(val_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "GRU_100_Tuning.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
